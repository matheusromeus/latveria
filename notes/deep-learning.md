
neural networks are a part of deep learning trying to emulate the brain.

the most simplest neural network is a Perceptron.

```
understand perceptron and multi-layered perceptron
```

z = weight (d.p) input + bias

to use images in this equation, we flatten the images into a list of pixel values.


cat or dog -> binary classification problem

Mean Squared Error?
- if this is low, then you can tell the model is good.

CNN - good for image recognition
LSTM - good for speech recognition


## Multi Layered Perceptron

Input layer, hidden layers, output layer




we have an input. and we have an output. but we do not know how the input turns into an output. we might have an idea of the relationship between the input and the output. that relationship is a linear function. so we assume that an go forward with the operation. the operation will result in an error. the error is calculated by expected value - actual value we got. using this error we can know how close we are or how far we are from the expected answer.



there are some values which correspond to X.
some values correspond to Y.

we need to find a linear function which can plot a line which classifies the two of them according to some parameters. 